# AI chat with ollama spring-boot and htmx

https://github.com/user-attachments/assets/055f0084-cc4d-43b5-8496-3971fdce5ae0.mov

## Usage

To run app:

```shell
docker compose up
```

For the first time you need to wait until llama3.1(~4GB) model will be downloaded.
Once the model downloaded, you can access app on http://localhost:8080/index.html

To stop app:

```shell
docker compose down
```
