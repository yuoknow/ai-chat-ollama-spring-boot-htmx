# AI chat with ollama spring-boot and htmx

![gif](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMDBjYjl5cjRwNHl4aGNib2htbjJxcW1oeWQ3dWFmdTgwZGttZHg3NCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/GVZWYOW5woUCo7YsMv/giphy.gif)

## Usage

To run app:

```shell
docker compose up
```

For the first time you need to wait until llama3.1(~4GB) model will be downloaded.
Once the model downloaded, you can access app on http://localhost:8080/index.html

To stop app:

```shell
docker compose down
```
