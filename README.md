# AI chat with ollama spring-boot and htmx
![question](https://github.com/user-attachments/assets/2dfcb2ef-1c33-46dc-8422-311b7b73fb04)


## Usage

To run app:

```shell
docker compose up
```

For the first time you need to wait until llama3.1(~4GB) model will be downloaded.
Once the model downloaded, you can access app on http://localhost:8080/index.html

To stop app:

```shell
docker compose down
```
