# AI chat with ollama spring-boot and htmx
## Usage

To run app:

```shell
docker compose up
```

For the first time you need to wait until llama3.1(~4GB) model will be downloaded.
Once the model downloaded, you can access app on http://localhost:8080/index.html

To stop app:

```shell
docker compose down
```